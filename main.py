import os
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup
import pdfplumber
import io
import re
import urllib3
from urllib.parse import urljoin
import json
import time

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
TG_BOT_TOKEN = os.environ.get("TG_BOT_TOKEN")
TG_CHAT_ID = os.environ.get("TG_CHAT_ID")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class MacroAgent:
    def __init__(self):
        self.history_file = "history.json"
        self.processed_urls = self.load_history()
        
        self.session = requests.Session()
        retries = Retry(total=3, backoff_factor=2, status_forcelist=[500, 502, 503, 504])
        self.session.mount('https://', HTTPAdapter(max_retries=retries))

        # 1. –°–ü–ò–°–û–ö –¶–ï–õ–ï–ô (–û—Ç—á–µ—Ç—ã)
        self.targets_cbr = [
            r"–û–±–∑–æ—Ä —Ä–∏—Å–∫–æ–≤",
            r"–†–µ–≥–∏–æ–Ω–∞–ª—å–Ω–∞—è —ç–∫–æ–Ω–æ–º–∏–∫–∞",
            r"–ú–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π –æ–ø—Ä–æ—Å",
            r"–î–µ–Ω–µ–∂–Ω–æ-–∫—Ä–µ–¥–∏—Ç–Ω—ã–µ —É—Å–ª–æ–≤–∏—è",
            r"–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ—Ç—Ä–∞—Å–ª–µ–≤—ã—Ö",
            r"–î–æ–∫–ª–∞–¥ –æ –¥–µ–Ω–µ–∂–Ω–æ-–∫—Ä–µ–¥–∏—Ç–Ω–æ–π"
        ]
        
        self.targets_minec = [
            r"–û —Ç–µ–∫—É—â–µ–π —Å–∏—Ç—É–∞—Ü–∏–∏",
            r"–ö–∞—Ä—Ç–∏–Ω–∞ –¥–µ–ª–æ–≤–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏",
            r"–≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π –æ–±–∑–æ—Ä"
        ]

        # 2. –§–ò–õ–¨–¢–† –î–ê–¢–´ (–°–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ!)
        # –ò—â–µ–º —Ç–æ–ª—å–∫–æ –∫–æ–Ω–µ—Ü –≥–æ–¥–∞ (–ù–æ—è–±—Ä—å, –î–µ–∫–∞–±—Ä—å 2025)
        # –≠—Ç–æ –∑–∞—Ö–≤–∞—Ç–∏—Ç "–≤—á–µ—Ä–∞", "—Å–µ–≥–æ–¥–Ω—è" –∏ "–Ω–µ–¥–µ–ª—é –Ω–∞–∑–∞–¥", –Ω–æ –æ—Ç—Å–µ—á–µ—Ç —Å—Ç–∞—Ä—å–µ.
        self.valid_dates = [
            "–¥–µ–∫–∞–±—Ä—è 2025", "–Ω–æ—è–±—Ä—è 2025",  # –¢–µ–∫—Å—Ç –Ω–∞ —Å–∞–π—Ç–µ (—Ä—É—Å)
            "12.2025", "11.2025",           # –î–∞—Ç—ã –≤ —Å—Å—ã–ª–∫–∞—Ö
            "2025-12", "2025-11",           # –§–æ—Ä–º–∞—Ç ISO
            "_12_25", "_11_25"              # –í –Ω–∞–∑–≤–∞–Ω–∏—è—Ö —Ñ–∞–π–ª–æ–≤
        ]

    def load_history(self):
        if os.path.exists(self.history_file):
            try:
                with open(self.history_file, 'r') as f:
                    return set(json.load(f))
            except:
                return set()
        return set()

    def save_history(self, url):
        self.processed_urls.add(url)
        with open(self.history_file, 'w') as f:
            json.dump(list(self.processed_urls), f)

    def send_telegram(self, message):
        if not TG_BOT_TOKEN or not TG_CHAT_ID: return
        print(f"üì§ TG Out: {message[:30]}...")
        for chunk in [message[i:i+4000] for i in range(0, len(message), 4000)]:
            url = f"https://api.telegram.org/bot{TG_BOT_TOKEN}/sendMessage"
            try:
                self.session.post(url, data={"chat_id": TG_CHAT_ID, "text": chunk, "parse_mode": "Markdown"}, timeout=15)
                time.sleep(1)
            except Exception as e:
                print(f"TG Error: {e}")

    def get_soup(self, url, source="generic"):
        # –ú–∞—Å–∫–∏—Ä–æ–≤–∫–∞ –ø–æ–¥ –Ø–Ω–¥–µ–∫—Å –¥–ª—è –≤—Å–µ—Ö
        headers = {
            "User-Agent": "Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
        }
        try:
            resp = self.session.get(url, headers=headers, verify=False, timeout=60)
            resp.encoding = resp.apparent_encoding
            return BeautifulSoup(resp.text, 'html.parser')
        except Exception as e:
            print(f"‚ö†Ô∏è –°–±–æ–π ({url}): {e}")
            return None

    def extract_text_from_pdf(self, pdf_url):
        print(f"‚¨áÔ∏è PDF: {pdf_url}")
        try:
            headers = {"User-Agent": "Mozilla/5.0 (compatible; YandexBot/3.0)"}
            resp = self.session.get(pdf_url, headers=headers, verify=False, timeout=60)
            with pdfplumber.open(io.BytesIO(resp.content)) as pdf:
                text = ""
                # –ß–∏—Ç–∞–µ–º 7 —Å—Ç—Ä–∞–Ω–∏—Ü
                for i in range(min(7, len(pdf.pages))):
                    t = pdf.pages[i].extract_text()
                    if t: text += t + "\n"
                return text
        except Exception as e:
            print(f"PDF Fail: {e}")
            return None

    def analyze_with_gpt(self, text, title, source_name):
        if not OPENAI_API_KEY: return "‚ö†Ô∏è –ù–µ—Ç AI –∫–ª—é—á–∞."
        print("üß† GPT –ê–Ω–∞–ª–∏–∑...")
        try:
            from openai import OpenAI
            client = OpenAI(api_key=OPENAI_API_KEY)
            prompt = f"""
            –¢—ã ‚Äî –º–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏—Å—Ç. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π: "{title}" ({source_name}).
            –î–∞–π —Å–∏–≥–Ω–∞–ª —Ç—Ä–µ–π–¥–µ—Ä—É –û–§–ó.
            –°–¢–†–£–ö–¢–£–†–ê:
            1. ü¶Ö **–†–∏—Ç–æ—Ä–∏–∫–∞:** (–ñ–µ—Å—Ç–∫–∞—è/–ú—è–≥–∫–∞—è).
            2. üìä **–§–∞–∫—Ç—ã:** (–ò–Ω—Ñ–ª—è—Ü–∏—è, –û–∂–∏–¥–∞–Ω–∏—è, –ö—Ä–µ–¥–∏—Ç—ã).
            3. üèõ **–í—ã–≤–æ–¥ –¥–ª—è –û–§–ó:** (–ü–æ–∫—É–ø–∞—Ç—å/–ü—Ä–æ–¥–∞–≤–∞—Ç—å).
            4. üî• **–†–∏—Å–∫:** (–ï—Å–ª–∏ –µ—Å—Ç—å).
            –¢–µ–∫—Å—Ç: {text[:12000]}
            """
            response = client.chat.completions.create(
                model="gpt-4o", messages=[{"role": "user", "content": prompt}], temperature=0.3
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"AI Error: {e}"

    def is_fresh(self, text_to_check):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞: —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Ç–µ–∫—Å—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –ù–æ—è–±—Ä—è –∏–ª–∏ –î–µ–∫–∞–±—Ä—è 2025"""
        if not text_to_check: return False
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º "2025"
        if "2025" not in text_to_check: return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–∞—Ä–∫–µ—Ä–æ–≤ –º–µ—Å—è—Ü–∞ (–Ω–æ—è–±—Ä—å/–¥–µ–∫–∞–±—Ä—å)
        # –≠—Ç–æ –æ—Ç—Å–µ—á–µ—Ç —è–Ω–≤–∞—Ä—å-–æ–∫—Ç—è–±—Ä—å 2025
        for date_marker in self.valid_dates:
            if date_marker in text_to_check:
                return True
        return False

    def check_cbr(self):
        print("üîç [–¶–ë] –ü—Ä–æ–≤–µ—Ä–∫–∞...")
        urls = ["https://www.cbr.ru/calendar"] # –ö–∞–ª–µ–Ω–¥–∞—Ä—å - —Å–∞–º–æ–µ –Ω–∞–¥–µ–∂–Ω–æ–µ
        
        for start_url in urls:
            soup = self.get_soup(start_url)
            if not soup: continue

            for link in soup.find_all('a'):
                title = link.get_text(strip=True)
                href = link.get('href')
                if not href or not title: continue
                
                # === –ì–õ–ê–í–ù–´–ô –§–ò–õ–¨–¢–† ===
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ –∏–ª–∏ —Å—Å—ã–ª–∫–µ –Ω—É–∂–Ω–∞—è –¥–∞—Ç–∞ (–ù–æ—è/–î–µ–∫ 2025)
                full_check_string = (title + href).lower()
                
                # –ï—Å–ª–∏ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ –Ω–µ—Ç 2025 - —Å—Ä–∞–∑—É –º–∏–º–æ
                if "2025" not in title: continue

                # –ï—Å–ª–∏ –Ω–µ—Ç –º–∞—Ä–∫–µ—Ä–æ–≤ –∫–æ–Ω—Ü–∞ –≥–æ–¥–∞ (—á—Ç–æ–±—ã –Ω–µ –±—Ä–∞—Ç—å —Å—Ç–∞—Ä—å–µ)
                is_fresh_date = any(d in title.lower() for d in ["–Ω–æ—è–±—Ä—è", "–¥–µ–∫–∞–±—Ä—è"])
                # –ï—Å–ª–∏ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ –Ω–µ—Ç –º–µ—Å—è—Ü–∞, –Ω–æ –µ—Å—Ç—å 2025 - –º–æ–∂–Ω–æ —Ä–∏—Å–∫–Ω—É—Ç—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å
                
                is_target = any(re.search(p, title, re.IGNORECASE) for p in self.targets_cbr)
                
                if is_target:
                    full_url = urljoin("https://www.cbr.ru", href)
                    if full_url in self.processed_urls: 
                        print(f"–ü—Ä–æ–ø—É—Å–∫ (—É–∂–µ –±—ã–ª–æ): {title}")
                        continue
                    
                    print(f"üî• –ù–ê–ô–î–ï–ù –ö–ê–ù–î–ò–î–ê–¢: {title}")
                    
                    # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ PDF
                    pdf_url = full_url if href.lower().endswith('.pdf') else None
                    if not pdf_url:
                        sub = self.get_soup(full_url)
                        if sub:
                            pl = sub.find('a', href=re.compile(r'\.pdf$', re.IGNORECASE))
                            if pl: pdf_url = urljoin("https://www.cbr.ru", pl['href'])
                    
                    if pdf_url:
                        text = self.extract_text_from_pdf(pdf_url)
                        if text:
                            # –î–æ–ø. –ø—Ä–æ–≤–µ—Ä–∫–∞: –∞ –≤–¥—Ä—É–≥ PDF —Å—Ç–∞—Ä—ã–π? (2022 –≥–æ–¥)
                            # –°–º–æ—Ç—Ä–∏–º –ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ PDF –Ω–∞ –Ω–∞–ª–∏—á–∏–µ 2025
                            if "2025" in text[:500] or "2025" in title:
                                ans = self.analyze_with_gpt(text, title, "–¶–ë –†–§")
                                self.send_telegram(f"üè¶ **–¶–ë –†–§**\n\nüìÑ {title}\n\n{ans}\nüîó {pdf_url}")
                                self.save_history(full_url)
                            else:
                                print("PDF –æ–∫–∞–∑–∞–ª—Å—è —Å—Ç–∞—Ä—ã–º (–Ω–µ 2025).")

    def check_minec(self):
        print("üîç [–ú–ò–ù–≠–ö] –ü—Ä–æ–≤–µ—Ä–∫–∞...")
        url = "https://www.economy.gov.ru/material/directions/makroec/ekonomicheskie_obzory/"
        soup = self.get_soup(url) 
        if not soup: return

        for link in soup.find_all('a'):
            title = link.get_text(strip=True)
            href = link.get('href')
            if not href or not title: continue
            
            # –§–∏–ª—å—Ç—Ä –Ω–∞ –ù–æ—è–±—Ä—å/–î–µ–∫–∞–±—Ä—å 2025
            if "2025" not in title: continue
            
            is_target = any(re.search(p, title, re.IGNORECASE) for p in self.targets_minec)
            if is_target:
                full_url = urljoin("https://www.economy.gov.ru", href)
                if full_url in self.processed_urls: continue
                
                print(f"üî• –ù–ê–ô–î–ï–ù –ú–ò–ù–≠–ö: {title}")
                sub = self.get_soup(full_url)
                if sub:
                    pl = sub.find('a', href=re.compile(r'\.pdf$', re.IGNORECASE))
                    if pl:
                        p_url = urljoin("https://www.economy.gov.ru", pl['href'])
                        text = self.extract_text_from_pdf(p_url)
                        if text:
                            ans = self.analyze_with_gpt(text, title, "–ú–∏–Ω–≠–∫")
                            self.send_telegram(f"üìâ **–ú–ò–ù–≠–ö**\n\nüìÑ {title}\n\n{ans}\nüîó {p_url}")
                            self.save_history(full_url)

    def run(self):
        self.check_cbr()
        self.check_minec()
        print("‚úÖ –ì–æ—Ç–æ–≤–æ")

if __name__ == "__main__":
    MacroAgent().run()
