import os
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup
import pdfplumber
import io
import re
import urllib3
from urllib.parse import urljoin
import time

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
TG_BOT_TOKEN = os.environ.get("TG_BOT_TOKEN")
TG_CHAT_ID = os.environ.get("TG_CHAT_ID")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class CBRAgent:
    def __init__(self):
        self.session = requests.Session()
        retries = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
        self.session.mount('https://', HTTPAdapter(max_retries=retries))
        
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
        }

        # –¶–µ–ª–∏ (—É–ø—Ä–æ—Å—Ç–∏–ª, –∏—â–µ–º —Å–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ)
        self.targets = [
            "–û–±–∑–æ—Ä —Ä–∏—Å–∫–æ–≤",
            "–†–µ–≥–∏–æ–Ω–∞–ª—å–Ω–∞—è —ç–∫–æ–Ω–æ–º–∏–∫–∞",
            "–ú–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π –æ–ø—Ä–æ—Å"
        ]

    def send_telegram(self, message):
        if not TG_BOT_TOKEN or not TG_CHAT_ID: return
        print(f"üì§ TG Out: {message[:50]}...")
        url = f"https://api.telegram.org/bot{TG_BOT_TOKEN}/sendMessage"
        try:
            self.session.post(url, data={"chat_id": TG_CHAT_ID, "text": message, "parse_mode": "Markdown"}, timeout=10)
        except Exception as e:
            print(f"TG Error: {e}")

    def get_soup(self, url):
        try:
            resp = self.session.get(url, headers=self.headers, verify=False, timeout=30)
            return BeautifulSoup(resp.text, 'html.parser')
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ {url}: {e}")
            return None

    def extract_text_from_pdf(self, pdf_url):
        print(f"‚¨áÔ∏è –ö–∞—á–∞–µ–º PDF: {pdf_url}")
        try:
            resp = self.session.get(pdf_url, headers=self.headers, verify=False, timeout=60)
            with pdfplumber.open(io.BytesIO(resp.content)) as pdf:
                text = ""
                for i in range(min(5, len(pdf.pages))):
                    t = pdf.pages[i].extract_text()
                    if t: text += t + "\n"
                return text
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF: {e}")
            return None

    def analyze_with_gpt(self, text, title):
        if not OPENAI_API_KEY: return "‚ö†Ô∏è –ù–µ—Ç –∫–ª—é—á–∞ AI."
        print("üß† GPT –ê–Ω–∞–ª–∏–∑...")
        try:
            from openai import OpenAI
            client = OpenAI(api_key=OPENAI_API_KEY)
            prompt = f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –æ—Ç—á–µ—Ç –¶–ë –†–§ '{title}'. –î–∞–π —Å–∏–≥–Ω–∞–ª –¥–ª—è –û–§–ó. –¢–µ–∫—Å—Ç: {text[:8000]}"
            response = client.chat.completions.create(
                model="gpt-4o", messages=[{"role": "user", "content": prompt}], temperature=0.3
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"GPT Error: {e}"

    def run(self):
        print("üîç –ó–ê–ü–£–°–ö –û–¢–õ–ê–î–ö–ò –¶–ë...")
        url = "https://www.cbr.ru/calendar"
        
        soup = self.get_soup(url)
        if not soup: return

        links = soup.find_all('a')
        print(f"–í—Å–µ–≥–æ —Å—Å—ã–ª–æ–∫ –≤ –∫–∞–ª–µ–Ω–¥–∞—Ä–µ: {len(links)}")

        for link in links:
            title = link.get_text(strip=True)
            href = link.get('href')
            
            if not href or not title: continue
            
            # –ò—â–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
            is_target = any(t.lower() in title.lower() for t in self.targets)
            
            if is_target:
                print(f"\nüéØ –¶–ï–õ–¨ –ù–ê–ô–î–ï–ù–ê –í –ö–ê–õ–ï–ù–î–ê–†–ï: {title}")
                print(f"   –°—Å—ã–ª–∫–∞: {href}")
                
                full_url = urljoin("https://www.cbr.ru", href)
                
                # –ó–∞—Ö–æ–¥–∏–º –≤–Ω—É—Ç—Ä—å
                print(f"   ‚û°Ô∏è –ó–∞—Ö–æ–¥–∏–º –≤–Ω—É—Ç—Ä—å: {full_url}")
                sub_soup = self.get_soup(full_url)
                
                if sub_soup:
                    # –í—ã–≤–æ–¥–∏–º –í–°–ï —Å—Å—ã–ª–∫–∏ –Ω–∞ PDF, –∫–æ—Ç–æ—Ä—ã–µ —Ç–∞–º –µ—Å—Ç—å
                    all_pdfs = sub_soup.find_all('a', href=re.compile(r'\.pdf$', re.IGNORECASE))
                    print(f"   üìÑ –ù–∞–π–¥–µ–Ω–æ PDF –≤–Ω—É—Ç—Ä–∏: {len(all_pdfs)}")
                    
                    for pdf in all_pdfs:
                        pdf_href = pdf['href']
                        print(f"      - {pdf_href}")
                        
                        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ 2025 –≥–æ–¥
                        if "2025" in pdf_href:
                            print("      ‚úÖ –≠–¢–û 2025! –û–ë–†–ê–ë–ê–¢–´–í–ê–ï–ú...")
                            target_pdf = urljoin("https://www.cbr.ru", pdf_href)
                            text = self.extract_text_from_pdf(target_pdf)
                            if text:
                                ans = self.analyze_with_gpt(text, title)
                                self.send_telegram(f"üêû **DEBUG MODE**\n\nüìÑ {title}\n\n{ans}\nüîó {target_pdf}")
                                return # –ü–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ —É—Å–ø–µ—Ö–∞ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è (–¥–ª—è —Ç–µ—Å—Ç–∞)
                        else:
                            print("      ‚ùå –ù–µ 2025 –≥–æ–¥, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º.")
                else:
                    print("   ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –Ω–æ–≤–æ—Å—Ç–∏.")

        print("\n‚úÖ –û—Ç–ª–∞–¥–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")

if __name__ == "__main__":
    CBRAgent().run()
