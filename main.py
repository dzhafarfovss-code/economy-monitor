import os
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup
import pdfplumber
import io
import re
import urllib3
from urllib.parse import urljoin
import json
import time

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
TG_BOT_TOKEN = os.environ.get("TG_BOT_TOKEN")
TG_CHAT_ID = os.environ.get("TG_CHAT_ID")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class CBRAgent:
    def __init__(self):
        self.session = requests.Session()
        retries = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
        self.session.mount('https://', HTTPAdapter(max_retries=retries))
        
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
        }

        self.targets = [
            "–û–±–∑–æ—Ä —Ä–∏—Å–∫–æ–≤",
            "–†–µ–≥–∏–æ–Ω–∞–ª—å–Ω–∞—è —ç–∫–æ–Ω–æ–º–∏–∫–∞",
            "–ú–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π –æ–ø—Ä–æ—Å",
            "–î–µ–Ω–µ–∂–Ω–æ-–∫—Ä–µ–¥–∏—Ç–Ω—ã–µ —É—Å–ª–æ–≤–∏—è",
            "–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ—Ç—Ä–∞—Å–ª–µ–≤—ã—Ö",
            "–î–æ–∫–ª–∞–¥ –æ –¥–µ–Ω–µ–∂–Ω–æ-–∫—Ä–µ–¥–∏—Ç–Ω–æ–π",
            "–ò–Ω—Ñ–ª—è—Ü–∏–æ–Ω–Ω—ã–µ –æ–∂–∏–¥–∞–Ω–∏—è"
        ]

    def send_telegram(self, message):
        if not TG_BOT_TOKEN or not TG_CHAT_ID: return
        
        url = f"https://api.telegram.org/bot{TG_BOT_TOKEN}/sendMessage"
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –¥–ª–∏–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        for chunk in [message[i:i+4000] for i in range(0, len(message), 4000)]:
            # –ü–û–ü–´–¢–ö–ê 1: –ö—Ä–∞—Å–∏–≤–æ (Markdown)
            data = {"chat_id": TG_CHAT_ID, "text": chunk, "parse_mode": "Markdown"}
            resp = self.session.post(url, data=data)
            
            # –ï—Å–ª–∏ –æ—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (400 Bad Request)
            if resp.status_code != 200:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ Markdown: {resp.text}. –ü—Ä–æ–±—É—é –æ–±—ã—á–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º...")
                # –ü–û–ü–´–¢–ö–ê 2: –û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç (–ë–µ–∑ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
                clean_text = chunk.replace("*", "").replace("_", "").replace("`", "")
                data = {"chat_id": TG_CHAT_ID, "text": clean_text} # –ë–µ–∑ parse_mode
                self.session.post(url, data=data)
            else:
                print("‚úÖ –°–æ–æ–±—â–µ–Ω–∏–µ –¥–æ—Å—Ç–∞–≤–ª–µ–Ω–æ.")
            
            time.sleep(1)

    def get_soup(self, url):
        try:
            resp = self.session.get(url, headers=self.headers, verify=False, timeout=30)
            return BeautifulSoup(resp.text, 'html.parser')
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ {url}: {e}")
            return None

    def extract_text_from_pdf(self, pdf_url):
        print(f"‚¨áÔ∏è –ö–∞—á–∞–µ–º PDF: {pdf_url}")
        try:
            resp = self.session.get(pdf_url, headers=self.headers, verify=False, timeout=60)
            with pdfplumber.open(io.BytesIO(resp.content)) as pdf:
                text = ""
                for i in range(min(7, len(pdf.pages))):
                    t = pdf.pages[i].extract_text()
                    if t: text += t + "\n"
                return text
        except:
            return None

    def analyze_with_gpt(self, text, title):
        if not OPENAI_API_KEY: return "‚ö†Ô∏è –ù–µ—Ç –∫–ª—é—á–∞ OpenAI."
        print("üß† GPT –ê–Ω–∞–ª–∏–∑...")
        try:
            from openai import OpenAI
            client = OpenAI(api_key=OPENAI_API_KEY)
            # –ü—Ä–æ—Å–∏–º GPT –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã Markdown
            prompt = f"""
            –¢—ã ‚Äî –º–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏—Å—Ç. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –æ—Ç—á–µ—Ç –¶–ë: "{title}".
            –î–∞–π —Å–∏–≥–Ω–∞–ª –¥–ª—è –û–§–ó. –ò—Å–ø–æ–ª—å–∑—É–π –º–∏–Ω–∏–º—É–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (—Ç–æ–ª—å–∫–æ –∑–≤–µ–∑–¥–æ—á–∫–∏ –¥–ª—è –∂–∏—Ä–Ω–æ–≥–æ).
            
            –°–¢–†–£–ö–¢–£–†–ê:
            1. *–†–∏—Ç–æ—Ä–∏–∫–∞:* (–ñ–µ—Å—Ç–∫–∞—è/–ú—è–≥–∫–∞—è).
            2. *–§–∞–∫—Ç—ã:* (–ò–Ω—Ñ–ª—è—Ü–∏—è, –ö—Ä–µ–¥–∏—Ç—ã).
            3. *–í—ã–≤–æ–¥ –û–§–ó:* (–ü–æ–∫—É–ø–∞—Ç—å/–ü—Ä–æ–¥–∞–≤–∞—Ç—å).
            4. *–†–∏—Å–∫:* –ì–ª–∞–≤–Ω–∞—è —É–≥—Ä–æ–∑–∞.

            –¢–µ–∫—Å—Ç: {text[:12000]}
            """
            response = client.chat.completions.create(
                model="gpt-4o", messages=[{"role": "user", "content": prompt}], temperature=0.3
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"GPT Error: {e}"

    def run(self):
        print("üîç –ü–æ–∏—Å–∫ –≤ –ö–∞–ª–µ–Ω–¥–∞—Ä–µ –¶–ë...")
        url = "https://www.cbr.ru/calendar"
        
        soup = self.get_soup(url)
        if not soup: return

        links = soup.find_all('a')
        for link in links:
            title = link.get_text(strip=True)
            href = link.get('href')
            
            if not href or not title: continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ
            is_target = any(re.search(p, title, re.IGNORECASE) for p in self.targets)
            
            if is_target:
                full_url = urljoin("https://www.cbr.ru", href)
                print(f"üîé –†–∞–∑–¥–µ–ª –Ω–∞–π–¥–µ–Ω: {title}")
                
                sub_soup = self.get_soup(full_url)
                if sub_soup:
                    # –°–æ–±–∏—Ä–∞–µ–º –í–°–ï PDF —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    pdf_links = sub_soup.find_all('a', href=re.compile(r'\.pdf$', re.IGNORECASE))
                    
                    found_pdf_url = None
                    
                    # –ü–†–ò–û–†–ò–¢–ï–¢ 1: –ò—â–µ–º –ù–æ—è–±—Ä—å 2025 (11-2025, 2025-11, 11_25)
                    for pl in pdf_links:
                        ref = pl['href']
                        if "2025" in ref and ("-11" in ref or "_11" in ref or "11_2025" in ref):
                            found_pdf_url = urljoin("https://www.cbr.ru", ref)
                            print("üî• –ù–ê–ô–î–ï–ù –ù–û–Ø–ë–†–¨–°–ö–ò–ô –û–¢–ß–ï–¢!")
                            break
                    
                    # –ü–†–ò–û–†–ò–¢–ï–¢ 2: –ï—Å–ª–∏ –Ω–æ—è–±—Ä—è –Ω–µ—Ç, –±–µ—Ä–µ–º –û–∫—Ç—è–±—Ä—å (10)
                    if not found_pdf_url:
                        for pl in pdf_links:
                            ref = pl['href']
                            if "2025" in ref and ("-10" in ref or "_10" in ref or "10_2025" in ref):
                                found_pdf_url = urljoin("https://www.cbr.ru", ref)
                                print("‚ÑπÔ∏è –ù–æ—è–±—Ä—è –Ω–µ—Ç, –±–µ—Ä–µ–º –û–∫—Ç—è–±—Ä—å.")
                                break
                    
                    if found_pdf_url:
                        text = self.extract_text_from_pdf(found_pdf_url)
                        if text:
                            ans = self.analyze_with_gpt(text, title)
                            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º! (–§—É–Ω–∫—Ü–∏—è —Å–∞–º–∞ —Ä–∞–∑–±–µ—Ä–µ—Ç—Å—è —Å —Ñ–æ—Ä–º–∞—Ç–æ–º)
                            self.send_telegram(f"üè¶ **–¶–ë –†–§**\n\nüìÑ {title}\n\n{ans}\nüîó {found_pdf_url}")
                            # –î–µ–ª–∞–µ–º –ø–∞—É–∑—É –∏ –≤—ã—Ö–æ–¥–∏–º (—á—Ç–æ–±—ã –Ω–µ —Å–ª–∞—Ç—å –¥—É–±–ª–∏ –æ–¥–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏)
                            time.sleep(2)

        print("‚úÖ –ì–æ—Ç–æ–≤–æ.")

if __name__ == "__main__":
    CBRAgent().run()
