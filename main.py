import os
import requests
from bs4 import BeautifulSoup
import pdfplumber
import io
import re
import urllib3
from urllib.parse import urljoin
import json
import time

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
TG_BOT_TOKEN = os.environ.get("TG_BOT_TOKEN")
TG_CHAT_ID = os.environ.get("TG_CHAT_ID")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class MacroAgent:
    def __init__(self):
        self.history_file = "history.json"
        self.processed_urls = self.load_history()
        
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }

        self.targets_cbr = [
            r"–û–±–∑–æ—Ä —Ä–∏—Å–∫–æ–≤",
            r"–†–µ–≥–∏–æ–Ω–∞–ª—å–Ω–∞—è —ç–∫–æ–Ω–æ–º–∏–∫–∞",
            r"–ú–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π –æ–ø—Ä–æ—Å",
            r"–î–µ–Ω–µ–∂–Ω–æ-–∫—Ä–µ–¥–∏—Ç–Ω—ã–µ —É—Å–ª–æ–≤–∏—è",
            r"–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ—Ç—Ä–∞—Å–ª–µ–≤—ã—Ö"
        ]

    def load_history(self):
        # –í –†–ï–ñ–ò–ú–ï –¢–ï–°–¢–ê –ò–ì–ù–û–†–ò–†–£–ï–ú –ò–°–¢–û–†–ò–Æ (—á—Ç–æ–±—ã –æ–Ω –ø—Ä–∏—Å–ª–∞–ª —Ç–æ, —á—Ç–æ —É–∂–µ –≤–∏–¥–µ–ª)
        return set()

    def save_history(self, url):
        pass # –í —Ç–µ—Å—Ç–µ –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º

    def send_telegram(self, message):
        if not TG_BOT_TOKEN or not TG_CHAT_ID:
            print("!!! –ù–ï–¢ –ö–õ–Æ–ß–ï–ô –¢–ï–õ–ï–ì–†–ê–ú–ê")
            return

        print(f"üì§ –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ Telegram: {message[:50]}...")
        url = f"https://api.telegram.org/bot{TG_BOT_TOKEN}/sendMessage"
        data = {"chat_id": TG_CHAT_ID, "text": message, "parse_mode": "Markdown"}
        try:
            requests.post(url, data=data, timeout=10)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ TG: {e}")

    def get_soup(self, url):
        try:
            resp = requests.get(url, headers=self.headers, verify=False, timeout=30)
            return BeautifulSoup(resp.text, 'html.parser')
        except:
            return None

    def extract_text_from_pdf(self, pdf_url):
        print(f"‚¨áÔ∏è –ö–∞—á–∞–µ–º: {pdf_url}")
        try:
            resp = requests.get(pdf_url, headers=self.headers, verify=False, timeout=60)
            with pdfplumber.open(io.BytesIO(resp.content)) as pdf:
                text = ""
                for i in range(min(5, len(pdf.pages))):
                    t = pdf.pages[i].extract_text()
                    if t: text += t + "\n"
                return text
        except Exception as e:
            print(f"PDF Fail: {e}")
            return None

    def analyze_with_gpt(self, text, title):
        if not OPENAI_API_KEY:
            return "‚ö†Ô∏è –ù–µ—Ç –∫–ª—é—á–∞ OpenAI."

        print("üß† GPT –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç...")
        try:
            from openai import OpenAI
            client = OpenAI(api_key=OPENAI_API_KEY)

            prompt = f"""
            –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–æ–∫—É–º–µ–Ω—Ç –¶–ë –†–§: "{title}".
            –î–∞–π –∫—Ä–∞—Ç–∫—É—é —Å—É—Ç—å –¥–ª—è —Ç—Ä–µ–π–¥–µ—Ä–∞ –û–§–ó (3 –ø—É–Ω–∫—Ç–∞).
            –¢–µ–∫—Å—Ç: {text[:8000]}
            """

            response = client.chat.completions.create(
                model="gpt-4o", 
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ GPT: {e}"

    def run(self):
        # 1. –ü–†–û–í–ï–†–ö–ê –°–í–Ø–ó–ò
        self.send_telegram("üëã **–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω!** –ù–∞—á–∏–Ω–∞—é –ø—Ä–æ–≤–µ—Ä–∫—É –¶–ë...")

        # 2. –ü–†–û–í–ï–†–ö–ê –¶–ë
        print("üîç –ò–¥–µ–º –Ω–∞ —Å–∞–π—Ç –¶–ë...")
        base_url = "https://www.cbr.ru"
        # –°–º–æ—Ç—Ä–∏–º —Ä–∞–∑–¥–µ–ª –∞–Ω–∞–ª–∏—Ç–∏–∫–∏, —Ç–∞–º —Å—Å—ã–ª–∫–∏ —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ
        start_url = "https://www.cbr.ru/analytics/fin_stab/" 
        
        soup = self.get_soup(start_url)
        if not soup:
            print("–°–∞–π—Ç –¶–ë –Ω–µ –æ—Ç–∫—Ä—ã–ª—Å—è")
            return

        links = soup.find_all('a')
        count = 0
        
        for link in links:
            title = link.get_text(strip=True)
            href = link.get('href')
            
            if not href or not title: continue
            
            # –ò—â–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—è–º
            is_target = any(re.search(p, title, re.IGNORECASE) for p in self.targets_cbr)
            
            if is_target:
                # –í –¢–ï–°–¢–ï –ë–ï–†–ï–ú –¢–û–õ–¨–ö–û –ü–ï–†–í–´–ô –ù–ê–ô–î–ï–ù–ù–´–ô –î–û–ö–£–ú–ï–ù–¢ –ò –°–¢–û–ü
                if count >= 1: break 
                
                full_url = urljoin(base_url, href)
                print(f"üî• –¢–µ—Å—Ç–æ–≤–∞—è –Ω–∞—Ö–æ–¥–∫–∞: {title}")
                
                pdf_url = full_url if href.endswith('.pdf') else None
                if not pdf_url:
                    # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ PDF –≤–Ω—É—Ç—Ä–∏
                    sub = self.get_soup(full_url)
                    if sub:
                        pl = sub.find('a', href=re.compile(r'\.pdf$', re.IGNORECASE))
                        if pl: pdf_url = urljoin(base_url, pl['href'])
                
                if pdf_url:
                    text = self.extract_text_from_pdf(pdf_url)
                    if text:
                        ans = self.analyze_with_gpt(text, title)
                        self.send_telegram(f"üß™ **–¢–ï–°–¢–û–í–´–ô –ü–†–û–ì–û–ù**\n\nüìÑ {title}\n\n{ans}\nüîó {pdf_url}")
                        count += 1

        if count == 0:
            self.send_telegram("–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–µ–ª –¥–∞–∂–µ –¥–ª—è —Ç–µ—Å—Ç–∞. –°—Ç—Ä–∞–Ω–Ω–æ.")

if __name__ == "__main__":
    MacroAgent().run()
