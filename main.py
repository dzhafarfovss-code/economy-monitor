import os
import requests
from bs4 import BeautifulSoup
import pdfplumber
import io
import re
import urllib3
from urllib.parse import urljoin
import json
import time

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
TG_BOT_TOKEN = os.environ.get("TG_BOT_TOKEN")
TG_CHAT_ID = os.environ.get("TG_CHAT_ID")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class MacroAgent:
    def __init__(self):
        self.history_file = "history.json"
        self.processed_urls = self.load_history()
        
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
        }

        self.targets_cbr = [
            r"–û–±–∑–æ—Ä —Ä–∏—Å–∫–æ–≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ä—ã–Ω–∫–æ–≤",
            r"–†–µ–≥–∏–æ–Ω–∞–ª—å–Ω–∞—è —ç–∫–æ–Ω–æ–º–∏–∫–∞",
            r"–ú–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π –æ–ø—Ä–æ—Å",
            r"–î–µ–Ω–µ–∂–Ω–æ-–∫—Ä–µ–¥–∏—Ç–Ω—ã–µ —É—Å–ª–æ–≤–∏—è",
            r"–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ—Ç—Ä–∞—Å–ª–µ–≤—ã—Ö —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –ø–æ—Ç–æ–∫–æ–≤",
            r"–î–æ–∫–ª–∞–¥ –æ –¥–µ–Ω–µ–∂–Ω–æ-–∫—Ä–µ–¥–∏—Ç–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–µ"
        ]
        
        self.targets_minec = [
            r"–û —Ç–µ–∫—É—â–µ–π —Å–∏—Ç—É–∞—Ü–∏–∏",
            r"–ö–∞—Ä—Ç–∏–Ω–∞ –¥–µ–ª–æ–≤–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏",
            r"–≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π –æ–±–∑–æ—Ä"
        ]

    def load_history(self):
        if os.path.exists(self.history_file):
            try:
                with open(self.history_file, 'r') as f:
                    return set(json.load(f))
            except:
                return set()
        return set()

    def save_history(self, url):
        self.processed_urls.add(url)
        with open(self.history_file, 'w') as f:
            json.dump(list(self.processed_urls), f)

    def send_telegram(self, message):
        if not TG_BOT_TOKEN or not TG_CHAT_ID:
            print("!!! TG Keys missing")
            return

        for chunk in [message[i:i+4000] for i in range(0, len(message), 4000)]:
            url = f"https://api.telegram.org/bot{TG_BOT_TOKEN}/sendMessage"
            data = {"chat_id": TG_CHAT_ID, "text": chunk, "parse_mode": "Markdown"}
            try:
                requests.post(url, data=data, timeout=10)
                time.sleep(1)
            except Exception as e:
                print(f"TG Error: {e}")

    def get_soup(self, url, timeout=20):
        try:
            resp = requests.get(url, headers=self.headers, verify=False, timeout=timeout)
            resp.raise_for_status()
            resp.encoding = resp.apparent_encoding
            return BeautifulSoup(resp.text, 'html.parser')
        except Exception as e:
            print(f"Connection Error ({url}): {e}")
            return None

    def extract_text_from_pdf(self, pdf_url):
        print(f"‚¨áÔ∏è PDF: {pdf_url}")
        try:
            resp = requests.get(pdf_url, headers=self.headers, verify=False, timeout=45)
            with pdfplumber.open(io.BytesIO(resp.content)) as pdf:
                text = ""
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–∞–Ω–∏—Ü (–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞)
                for i in range(min(5, len(pdf.pages))):
                    t = pdf.pages[i].extract_text()
                    if t: text += t + "\n"
                return text
        except Exception as e:
            print(f"PDF Error: {e}")
            return None

    def analyze_with_gpt(self, text, title, source_name):
        if not OPENAI_API_KEY:
            return "‚ö†Ô∏è AI Key missing. Text start:\n" + text[:500]

        print("üß† AI Analysis...")
        try:
            from openai import OpenAI
            client = OpenAI(api_key=OPENAI_API_KEY)

            prompt = f"""
            –¢—ã ‚Äî –º–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏—Å—Ç. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π: "{title}" ({source_name}).
            –î–∞–π –∫—Ä–∞—Ç–∫—É—é –≤—ã–∂–∏–º–∫—É –¥–ª—è –∏–Ω–≤–µ—Å—Ç–æ—Ä–∞ –≤ –û–§–ó –∏ –†—É–±–ª—å.
            
            –°–¢–†–£–ö–¢–£–†–ê:
            1. ü¶Ö **–†–∏—Ç–æ—Ä–∏–∫–∞:** (–ñ–µ—Å—Ç–∫–∞—è/–ú—è–≥–∫–∞—è).
            2. üìä **–ì–ª–∞–≤–Ω—ã–µ —Ü–∏—Ñ—Ä—ã:** (–ò–Ω—Ñ–ª—è—Ü–∏—è, –æ–∂–∏–¥–∞–Ω–∏—è, –∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏–µ).
            3. üèõ **–í—ã–≤–æ–¥ –¥–ª—è –û–§–ó:** (–ü–æ–∫—É–ø–∞—Ç—å/–ü—Ä–æ–¥–∞–≤–∞—Ç—å).
            4. üî• **–†–∏—Å–∫–∏:** (–ï—Å–ª–∏ –µ—Å—Ç—å).

            –¢–µ–∫—Å—Ç:
            {text[:10000]}
            """

            response = client.chat.completions.create(
                model="gpt-4o", 
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"AI Error: {e}"

    def check_cbr(self):
        print("üîç [–¶–ë] –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ...")
        # –°–º–æ—Ç—Ä–∏–º –Ω–µ —Ç–æ–ª—å–∫–æ –∫–∞–ª–µ–Ω–¥–∞—Ä—å, –Ω–æ –∏ —Ä–∞–∑–¥–µ–ª –∞–Ω–∞–ª–∏—Ç–∏–∫–∏, —Ç–∞–º –Ω–∞–¥–µ–∂–Ω–µ–µ
        urls_to_check = [
            "https://www.cbr.ru/calendar",
            "https://www.cbr.ru/analytics/dkp/hosting/", # –û–±–∑–æ—Ä—ã –î–ö–ü
            "https://www.cbr.ru/analytics/fin_stab/"     # –û–±–∑–æ—Ä—ã —Ä–∏—Å–∫–æ–≤
        ]
        
        for base_section in urls_to_check:
            soup = self.get_soup(base_section)
            if not soup: continue

            links = soup.find_all('a')
            for link in links:
                title = link.get_text(strip=True)
                href = link.get('href')
                
                if not href or not title: continue
                
                # === –§–ò–õ–¨–¢–† 2025 –ì–û–î–ê ===
                # –ï—Å–ª–∏ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –∏–ª–∏ —Å—Å—ã–ª–∫–µ –Ω–µ—Ç "2025" - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                if "2025" not in title and "2025" not in href:
                    continue

                is_target = any(re.search(p, title, re.IGNORECASE) for p in self.targets_cbr)
                
                if is_target:
                    full_url = urljoin("https://www.cbr.ru", href)
                    if full_url in self.processed_urls: continue
                    
                    print(f"üî• –ù–ê–ô–î–ï–ù –°–í–ï–ñ–ò–ô –û–¢–ß–ï–¢: {title}")
                    
                    pdf_url = None
                    if href.lower().endswith('.pdf'):
                        pdf_url = full_url
                    else:
                        sub = self.get_soup(full_url)
                        if sub:
                            pl = sub.find('a', href=re.compile(r'\.pdf$', re.IGNORECASE))
                            if pl: pdf_url = urljoin("https://www.cbr.ru", pl['href'])
                    
                    if pdf_url:
                        text = self.extract_text_from_pdf(pdf_url)
                        if text:
                            ans = self.analyze_with_gpt(text, title, "–¶–ë –†–§")
                            self.send_telegram(f"üè¶ **–¶–ë –†–§ (2025)**\n\nüìÑ {title}\n\n{ans}\nüîó {pdf_url}")
                            self.save_history(full_url)

    def check_minec(self):
        print("üîç [–ú–ò–ù–≠–ö] –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ...")
        url = "https://www.economy.gov.ru/material/directions/makroec/ekonomicheskie_obzory/"
        # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π —Ç–∞–π–º–∞—É—Ç –¥–ª—è –ú–∏–Ω—ç–∫–∞
        soup = self.get_soup(url, timeout=45) 
        if not soup: return

        links = soup.find_all('a')
        for link in links:
            title = link.get_text(strip=True)
            href = link.get('href')
            if not href or not title: continue
            
            # –¢–æ–∂–µ —Ñ–∏–ª—å—Ç—Ä—É–µ–º 2025
            if "2025" not in title and "2025" not in href: continue

            is_target = any(re.search(p, title, re.IGNORECASE) for p in self.targets_minec)
            if is_target:
                full_url = urljoin("https://www.economy.gov.ru", href)
                if full_url in self.processed_urls: continue
                
                print(f"üî• –ù–ê–ô–î–ï–ù –ú–ò–ù–≠–ö: {title}")
                sub = self.get_soup(full_url, timeout=45)
                if sub:
                    pl = sub.find('a', href=re.compile(r'\.pdf$', re.IGNORECASE))
                    if pl:
                        p_url = urljoin("https://www.economy.gov.ru", pl['href'])
                        text = self.extract_text_from_pdf(p_url)
                        if text:
                            ans = self.analyze_with_gpt(text, title, "–ú–∏–Ω–≠–∫")
                            self.send_telegram(f"üìâ **–ú–ò–ù–≠–ö**\n\nüìÑ {title}\n\n{ans}\nüîó {p_url}")
                            self.save_history(full_url)

    def run(self):
        self.check_cbr()
        self.check_minec()
        print("‚úÖ –ì–æ—Ç–æ–≤–æ")

if __name__ == "__main__":
    MacroAgent().run()
