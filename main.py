import os
import requests
from bs4 import BeautifulSoup
import pdfplumber
import io
import re
import urllib3
from urllib.parse import urljoin
import json
import time

# --- –ù–ê–°–¢–†–û–ô–ö–ò (–ë–µ—Ä–µ–º –∏–∑ GitHub Secrets) ---
TG_BOT_TOKEN = os.environ.get("TG_BOT_TOKEN")
TG_CHAT_ID = os.environ.get("TG_CHAT_ID")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è SSL (–¥–ª—è –ú–∏–Ω—ç–∫–∞ —á–∞—Å—Ç–æ –Ω—É–∂–Ω–æ)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class MacroAgent:
    def __init__(self):
        self.history_file = "history.json"
        self.processed_urls = self.load_history()
        
        # –ó–∞–≥–æ–ª–æ–≤–∫–∏, —á—Ç–æ–±—ã —Å–∞–π—Ç—ã –¥—É–º–∞–ª–∏, —á—Ç–æ –º—ã –±—Ä–∞—É–∑–µ—Ä
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
        }

        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ (–†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è)
        self.targets_cbr = [
            r"–û–±–∑–æ—Ä —Ä–∏—Å–∫–æ–≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ä—ã–Ω–∫–æ–≤",
            r"–†–µ–≥–∏–æ–Ω–∞–ª—å–Ω–∞—è —ç–∫–æ–Ω–æ–º–∏–∫–∞",
            r"–ú–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π –æ–ø—Ä–æ—Å",
            r"–î–µ–Ω–µ–∂–Ω–æ-–∫—Ä–µ–¥–∏—Ç–Ω—ã–µ —É—Å–ª–æ–≤–∏—è",
            r"–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ—Ç—Ä–∞—Å–ª–µ–≤—ã—Ö —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –ø–æ—Ç–æ–∫–æ–≤"
        ]
        
        self.targets_minec = [
            r"–û —Ç–µ–∫—É—â–µ–π —Å–∏—Ç—É–∞—Ü–∏–∏",
            r"–ö–∞—Ä—Ç–∏–Ω–∞ –¥–µ–ª–æ–≤–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏",
            r"–≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π –æ–±–∑–æ—Ä"
        ]

    def load_history(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —É–∂–µ –ø—Ä–æ—á–∏—Ç–∞–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫"""
        if os.path.exists(self.history_file):
            try:
                with open(self.history_file, 'r') as f:
                    return set(json.load(f))
            except:
                return set()
        return set()

    def save_history(self, url):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Å—ã–ª–∫—É, —á—Ç–æ–±—ã –Ω–µ —Å–ø–∞–º–∏—Ç—å –ø–æ–≤—Ç–æ—Ä–Ω–æ"""
        self.processed_urls.add(url)
        with open(self.history_file, 'w') as f:
            json.dump(list(self.processed_urls), f)

    def send_telegram(self, message):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –≤ Telegram —Å —Ä–∞–∑–±–∏–≤–∫–æ–π –¥–ª–∏–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π"""
        if not TG_BOT_TOKEN or not TG_CHAT_ID:
            print("!!! –û–®–ò–ë–ö–ê: –ù–µ—Ç –∫–ª—é—á–µ–π Telegram –≤ Secrets")
            return

        # –¢–µ–ª–µ–≥—Ä–∞–º –Ω–µ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª–∏–Ω–Ω–µ–µ 4096 —Å–∏–º–≤–æ–ª–æ–≤
        for chunk in [message[i:i+4000] for i in range(0, len(message), 4000)]:
            url = f"https://api.telegram.org/bot{TG_BOT_TOKEN}/sendMessage"
            data = {
                "chat_id": TG_CHAT_ID, 
                "text": chunk, 
                "parse_mode": "Markdown"
            }
            try:
                requests.post(url, data=data, timeout=10)
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ TG: {e}")

    def get_soup(self, url):
        """–°–∫–∞—á–∏–≤–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ –¥–µ–ª–∞–µ—Ç –°—É–ø"""
        try:
            resp = requests.get(url, headers=self.headers, verify=False, timeout=20)
            resp.raise_for_status()
            resp.encoding = resp.apparent_encoding
            return BeautifulSoup(resp.text, 'html.parser')
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ {url}: {e}")
            return None

    def extract_text_from_pdf(self, pdf_url):
        """–ö–∞—á–∞–µ—Ç PDF –∏ –≤—ã—Ç–∞—Å–∫–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –ø–µ—Ä–≤—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü"""
        print(f"‚¨áÔ∏è –°–∫–∞—á–∏–≤–∞–µ–º PDF: {pdf_url}")
        try:
            resp = requests.get(pdf_url, headers=self.headers, verify=False, timeout=30)
            with pdfplumber.open(io.BytesIO(resp.content)) as pdf:
                text = ""
                # –ß–∏—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 7 —Å—Ç—Ä–∞–Ω–∏—Ü (—Ç–∞–º —Å—É—Ç—å, –¥–∞–ª—å—à–µ —Ç–∞–±–ª–∏—Ü—ã)
                max_pages = min(7, len(pdf.pages))
                for i in range(max_pages):
                    page_text = pdf.pages[i].extract_text()
                    if page_text:
                        text += page_text + "\n"
                return text
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF: {e}")
            return None

    def analyze_with_gpt(self, text, title, source_name):
        """–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ OpenAI"""
        if not OPENAI_API_KEY:
            return "‚ö†Ô∏è –ö–ª—é—á OpenAI –Ω–µ –Ω–∞–π–¥–µ–Ω. –í–æ—Ç –Ω–∞—á–∞–ª–æ —Ç–µ–∫—Å—Ç–∞:\n" + text[:600] + "..."

        print("üß† –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ GPT...")
        try:
            from openai import OpenAI
            client = OpenAI(api_key=OPENAI_API_KEY)

            prompt = f"""
            –¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π –º–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏—Å—Ç –∏ —Ç—Ä–µ–π–¥–µ—Ä –æ–±–ª–∏–≥–∞—Ü–∏—è–º–∏ (–û–§–ó).
            –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–æ–∫—É–º–µ–Ω—Ç: "{title}" –æ—Ç {source_name}.
            
            –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –¥–∞—Ç—å —á–µ—Ç–∫–∏–µ —Å–∏–≥–Ω–∞–ª—ã –¥–ª—è —Ä—ã–Ω–∫–∞. –ù–µ –ª–µ–π –≤–æ–¥—É.
            
            –°–¢–†–£–ö–¢–£–†–ê –û–¢–í–ï–¢–ê:
            1. ü¶Ö **–†–∏—Ç–æ—Ä–∏–∫–∞:** (–ñ–µ—Å—Ç–∫–∞—è / –£–º–µ—Ä–µ–Ω–Ω–æ-–∂–µ—Å—Ç–∫–∞—è / –ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è / –ú—è–≥–∫–∞—è). –ü–æ—á–µ–º—É? (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è).
            2. üìà **–ò–Ω—Ñ–ª—è—Ü–∏—è –∏ –°—Ç–∞–≤–∫–∞:** –ì–ª–∞–≤–Ω—ã–µ —Ü–∏—Ñ—Ä—ã –∏ –ø—Ä–æ–≥–Ω–æ–∑. –ï—Å—Ç—å –ª–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∑–∞–º–µ–¥–ª–µ–Ω–∏—è –∏–Ω—Ñ–ª—è—Ü–∏–∏ –∏–ª–∏ –ø–µ—Ä–µ–≥—Ä–µ–≤–∞?
            3. üèõ **–û–§–ó –∏ –†—ã–Ω–æ–∫:** –ß—Ç–æ –¥–µ–ª–∞—Ç—å —Å –≥–æ—Å–æ–±–ª–∏–≥–∞—Ü–∏—è–º–∏? (–ü–æ–∫—É–ø–∞—Ç—å –∫–æ—Ä–æ—Ç–∫–∏–µ/–¥–ª–∏–Ω–Ω—ã–µ, –ø—Ä–æ–¥–∞–≤–∞—Ç—å, –¥–µ—Ä–∂–∞—Ç—å). –†–∏—Å–∫–∏?
            4. üî• **–í–∞–∂–Ω–æ–µ:** –ï—Å–ª–∏ –µ—Å—Ç—å —á—Ç–æ-—Ç–æ —ç–∫—Å—Ç—Ä–∞–æ—Ä–¥–∏–Ω–∞—Ä–Ω–æ–µ (—Ä–µ–∫–æ—Ä–¥–Ω—ã–π –¥–µ—Ñ–∏—Ü–∏—Ç –∫–∞–¥—Ä–æ–≤, –æ–±–≤–∞–ª —ç–∫—Å–ø–æ—Ä—Ç–∞ –∏ —Ç.–¥.).

            –¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–Ω–∞—á–∞–ª–æ):
            {text[:12000]}
            """

            response = client.chat.completions.create(
                model="gpt-4o", 
                messages=[{"role": "user", "content": prompt}],
                temperature=0.4
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ GPT: {e}. –°—ã—Ä–æ–π —Ç–µ–∫—Å—Ç: {text[:500]}..."

    # --- –õ–û–ì–ò–ö–ê –¶–ë –†–§ ---
    def check_cbr(self):
        print("üîç [–¶–ë –†–§] –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞–ª–µ–Ω–¥–∞—Ä—è...")
        base_url = "https://www.cbr.ru"
        calendar_url = "https://www.cbr.ru/calendar"
        
        soup = self.get_soup(calendar_url)
        if not soup: return

        links = soup.find_all('a')
        
        for link in links:
            title = link.get_text(strip=True)
            href = link.get('href')
            
            if not href or not title: continue
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            is_target = any(re.search(pattern, title, re.IGNORECASE) for pattern in self.targets_cbr)
            
            if is_target:
                full_url = urljoin(base_url, href)
                if full_url in self.processed_urls: continue
                
                print(f"üî• –ù–ê–ô–î–ï–ù –û–¢–ß–ï–¢ –¶–ë: {title}")
                
                # –ò—â–µ–º PDF
                pdf_url = None
                
                # –ï—Å–ª–∏ —Å—Å—ã–ª–∫–∞ —Å—Ä–∞–∑—É –Ω–∞ PDF
                if href.lower().endswith('.pdf'):
                    pdf_url = full_url
                else:
                    # –ó–∞—Ö–æ–¥–∏–º –≤–Ω—É—Ç—Ä—å –Ω–æ–≤–æ—Å—Ç–∏
                    sub_soup = self.get_soup(full_url)
                    if sub_soup:
                        pdf_link = sub_soup.find('a', href=re.compile(r'\.pdf$', re.IGNORECASE))
                        if pdf_link:
                            pdf_url = urljoin(base_url, pdf_link['href'])

                if pdf_url:
                    text = self.extract_text_from_pdf(pdf_url)
                    if text:
                        analysis = self.analyze_with_gpt(text, title, "–ë–∞–Ω–∫–∞ –†–æ—Å—Å–∏–∏")
                        msg = f"üè¶ **–¶–ë –†–§: –í–´–®–ï–õ –û–¢–ß–ï–¢**\n\nüìÑ *{title}*\n\n{analysis}\n\nüîó [–î–æ–∫—É–º–µ–Ω—Ç]({pdf_url})"
                        self.send_telegram(msg)
                        self.save_history(full_url)

    # --- –õ–û–ì–ò–ö–ê –ú–ò–ù–≠–ö (–†–æ—Å—Å—Ç–∞—Ç –¥–∞–Ω–Ω—ã–µ) ---
    def check_minec(self):
        print("üîç [–ú–ò–ù–≠–ö] –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±–∑–æ—Ä–æ–≤...")
        base_url = "https://www.economy.gov.ru"
        section_url = "https://www.economy.gov.ru/material/directions/makroec/ekonomicheskie_obzory/"
        
        soup = self.get_soup(section_url)
        if not soup: return

        # –ú–∏–Ω—ç–∫ —á–∞—Å—Ç–æ –º–µ–Ω—è–µ—Ç –≤–µ—Ä—Å—Ç–∫—É, –∏—â–µ–º –ø—Ä–æ—Å—Ç–æ —Å—Å—ã–ª–∫–∏ —Å –Ω—É–∂–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º
        links = soup.find_all('a')
        
        for link in links:
            title = link.get_text(strip=True)
            href = link.get('href')
            
            if not href or not title: continue
            
            is_target = any(re.search(pattern, title, re.IGNORECASE) for pattern in self.targets_minec)
            
            if is_target:
                full_url = urljoin(base_url, href)
                if full_url in self.processed_urls: continue
                
                print(f"üî• –ù–ê–ô–î–ï–ù –û–ë–ó–û–† –ú–ò–ù–≠–ö–ê: {title}")
                
                # –ó–∞—Ö–æ–¥–∏–º –≤–Ω—É—Ç—Ä—å
                sub_soup = self.get_soup(full_url)
                if sub_soup:
                    pdf_link = sub_soup.find('a', href=re.compile(r'\.pdf$', re.IGNORECASE))
                    if pdf_link:
                        pdf_url = urljoin(base_url, pdf_link['href'])
                        text = self.extract_text_from_pdf(pdf_url)
                        
                        if text:
                            analysis = self.analyze_with_gpt(text, title, "–ú–∏–Ω—ç–∫–æ–Ω–æ–º—Ä–∞–∑–≤–∏—Ç–∏—è")
                            msg = f"üìâ **–ú–ò–ù–≠–ö (–î–ê–ù–ù–´–ï –†–û–°–°–¢–ê–¢–ê)**\n\nüìÑ *{title}*\n\n{analysis}\n\nüîó [–î–æ–∫—É–º–µ–Ω—Ç]({pdf_url})"
                            self.send_telegram(msg)
                            self.save_history(full_url)

    def run(self):
        self.check_cbr()
        self.check_minec()
        print("‚úÖ –¶–∏–∫–ª –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–∞–≤–µ—Ä—à–µ–Ω.")

if __name__ == "__main__":
    agent = MacroAgent()
    agent.run()
