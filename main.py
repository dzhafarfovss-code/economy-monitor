import os
import requests
from bs4 import BeautifulSoup
import pdfplumber
import io
import re
import urllib3
from urllib.parse import urljoin
import json
import time

# --- –ù–ê–°–¢–†–û–ô–ö–ò (–ë–µ—Ä–µ–º –∏–∑ GitHub Secrets) ---
# –ï—Å–ª–∏ –∫–ª—é—á–µ–π –Ω–µ—Ç, —Å–∫—Ä–∏–ø—Ç –Ω–µ —É–ø–∞–¥–µ—Ç, –∞ –ø—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—à–µ—Ç –æ—à–∏–±–∫—É –≤ –ª–æ–≥
TG_BOT_TOKEN = os.environ.get("TG_BOT_TOKEN")
TG_CHAT_ID = os.environ.get("TG_CHAT_ID")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è SSL (—Å–∞–π—Ç—ã –≥–æ—Å–æ—Ä–≥–∞–Ω–æ–≤ —á–∞—Å—Ç–æ –∏–º–µ—é—Ç –∫—Ä–∏–≤—ã–µ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç—ã)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class MacroAgent:
    def __init__(self):
        self.history_file = "history.json"
        self.processed_urls = self.load_history()
        
        # –ü—Ä–∏—Ç–≤–æ—Ä—è–µ–º—Å—è –æ–±—ã—á–Ω—ã–º –±—Ä–∞—É–∑–µ—Ä–æ–º
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
        }

        # –ß–¢–û –ò–©–ï–ú –£ –¶–ë (–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞)
        self.targets_cbr = [
            r"–û–±–∑–æ—Ä —Ä–∏—Å–∫–æ–≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ä—ã–Ω–∫–æ–≤",
            r"–†–µ–≥–∏–æ–Ω–∞–ª—å–Ω–∞—è —ç–∫–æ–Ω–æ–º–∏–∫–∞",
            r"–ú–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π –æ–ø—Ä–æ—Å",
            r"–î–µ–Ω–µ–∂–Ω–æ-–∫—Ä–µ–¥–∏—Ç–Ω—ã–µ —É—Å–ª–æ–≤–∏—è",
            r"–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ—Ç—Ä–∞—Å–ª–µ–≤—ã—Ö —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –ø–æ—Ç–æ–∫–æ–≤",
            r"–î–æ–∫–ª–∞–¥ –æ –¥–µ–Ω–µ–∂–Ω–æ-–∫—Ä–µ–¥–∏—Ç–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–µ"
        ]
        
        # –ß–¢–û –ò–©–ï–ú –£ –ú–ò–ù–≠–ö–ê
        self.targets_minec = [
            r"–û —Ç–µ–∫—É—â–µ–π —Å–∏—Ç—É–∞—Ü–∏–∏",
            r"–ö–∞—Ä—Ç–∏–Ω–∞ –¥–µ–ª–æ–≤–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏",
            r"–≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π –æ–±–∑–æ—Ä"
        ]

    def load_history(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫"""
        if os.path.exists(self.history_file):
            try:
                with open(self.history_file, 'r') as f:
                    return set(json.load(f))
            except:
                return set()
        return set()

    def save_history(self, url):
        """–ó–∞–ø–æ–º–∏–Ω–∞–µ—Ç —Å—Å—ã–ª–∫—É, —á—Ç–æ–±—ã –Ω–µ —Å–ª–∞—Ç—å –ø–æ–≤—Ç–æ—Ä—ã"""
        self.processed_urls.add(url)
        with open(self.history_file, 'w') as f:
            json.dump(list(self.processed_urls), f)

    def send_telegram(self, message):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –≤ Telegram"""
        if not TG_BOT_TOKEN or not TG_CHAT_ID:
            print("!!! TG –ö–ª—é—á–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ Secrets")
            return

        # –¢–µ–ª–µ–≥—Ä–∞–º –Ω–µ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª–∏–Ω–Ω–µ–µ 4096 —Å–∏–º–≤–æ–ª–æ–≤, —Ä–µ–∂–µ–º –Ω–∞ –∫—É—Å–∫–∏
        for chunk in [message[i:i+4000] for i in range(0, len(message), 4000)]:
            url = f"https://api.telegram.org/bot{TG_BOT_TOKEN}/sendMessage"
            data = {
                "chat_id": TG_CHAT_ID, 
                "text": chunk, 
                "parse_mode": "Markdown"
            }
            try:
                requests.post(url, data=data, timeout=10)
                time.sleep(1) # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ TG: {e}")

    def get_soup(self, url):
        """–°–∫–∞—á–∏–≤–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É"""
        try:
            resp = requests.get(url, headers=self.headers, verify=False, timeout=20)
            resp.raise_for_status()
            # –§–∏–∫—Å –∫–æ–¥–∏—Ä–æ–≤–∫–∏
            resp.encoding = resp.apparent_encoding
            return BeautifulSoup(resp.text, 'html.parser')
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ {url}: {e}")
            return None

    def extract_text_from_pdf(self, pdf_url):
        """–ö–∞—á–∞–µ—Ç PDF –∏ —á–∏—Ç–∞–µ—Ç —Ç–µ–∫—Å—Ç (–ø–µ—Ä–≤—ã–µ 7 —Å—Ç—Ä–∞–Ω–∏—Ü)"""
        print(f"‚¨áÔ∏è –°–∫–∞—á–∏–≤–∞–µ–º PDF: {pdf_url}")
        try:
            resp = requests.get(pdf_url, headers=self.headers, verify=False, timeout=30)
            with pdfplumber.open(io.BytesIO(resp.content)) as pdf:
                text = ""
                # –ß–∏—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 7 —Å—Ç—Ä–∞–Ω–∏—Ü (—Ç–∞–º –≤—Å—è —Å—É—Ç—å –¥–ª—è —Ç—Ä–µ–π–¥–∏–Ω–≥–∞)
                max_pages = min(7, len(pdf.pages))
                for i in range(max_pages):
                    page_text = pdf.pages[i].extract_text()
                    if page_text:
                        text += page_text + "\n"
                return text
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF: {e}")
            return None

    def analyze_with_gpt(self, text, title, source_name):
        """–ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ GPT-4o"""
        if not OPENAI_API_KEY:
            return "‚ö†Ô∏è –ö–ª—é—á OpenAI –Ω–µ –Ω–∞–π–¥–µ–Ω. –í–æ—Ç –Ω–∞—á–∞–ª–æ —Ç–µ–∫—Å—Ç–∞:\n" + text[:600] + "..."

        print("üß† –î—É–º–∞–µ–º (–∑–∞–ø—Ä–æ—Å –∫ AI)...")
        try:
            from openai import OpenAI
            client = OpenAI(api_key=OPENAI_API_KEY)

            prompt = f"""
            –¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π –º–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏—Å—Ç –∏ —Ç—Ä–µ–π–¥–µ—Ä –æ–±–ª–∏–≥–∞—Ü–∏—è–º–∏ (–û–§–ó).
            –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–æ–∫—É–º–µ–Ω—Ç: "{title}" –æ—Ç {source_name}.
            
            –î–∞–π –°–£–•–£–Æ –≤—ã–∂–∏–º–∫—É –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π.
            
            –°–¢–†–£–ö–¢–£–†–ê –û–¢–í–ï–¢–ê:
            1. ü¶Ö **–†–∏—Ç–æ—Ä–∏–∫–∞:** (–ñ–µ—Å—Ç–∫–∞—è/–ú—è–≥–∫–∞—è/–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è). –ü–æ—á–µ–º—É?
            2. üìä **–ì–ª–∞–≤–Ω—ã–µ —Ü–∏—Ñ—Ä—ã:** (–ò–Ω—Ñ–ª—è—Ü–∏—è, –æ–∂–∏–¥–∞–Ω–∏—è, –∫–∞–¥—Ä–æ–≤—ã–π –≥–æ–ª–æ–¥, –ø–æ—Ç–æ–∫–∏ –≤ –û–§–ó).
            3. üèõ **–í–ª–∏—è–Ω–∏–µ –Ω–∞ –û–§–ó:** (–ü–æ–∫—É–ø–∞—Ç—å/–ü—Ä–æ–¥–∞–≤–∞—Ç—å/–î–µ—Ä–∂–∞—Ç—å). –ï—Å—Ç—å –ª–∏ —Å–º–µ–Ω–∞ —Ç—Ä–µ–Ω–¥–∞?
            4. üî• **–†–∏—Å–∫–∏:** –ß—Ç–æ –º–æ–∂–µ—Ç –ø–æ–π—Ç–∏ –Ω–µ —Ç–∞–∫?

            –¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–ø–µ—Ä–≤—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã):
            {text[:12000]}
            """

            response = client.chat.completions.create(
                model="gpt-4o", 
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ GPT: {e}. –°—ã—Ä–æ–π —Ç–µ–∫—Å—Ç: {text[:500]}..."

    # --- –ü–†–û–í–ï–†–ö–ê –¶–ë –†–§ (–ö–∞–ª–µ–Ω–¥–∞—Ä—å —Å–æ–±—ã—Ç–∏–π) ---
    def check_cbr(self):
        print("üîç [–¶–ë –†–§] –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞–ª–µ–Ω–¥–∞—Ä—è...")
        base_url = "https://www.cbr.ru"
        calendar_url = "https://www.cbr.ru/calendar"
        
        soup = self.get_soup(calendar_url)
        if not soup: return

        # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        links = soup.find_all('a')
        
        for link in links:
            title = link.get_text(strip=True)
            href = link.get('href')
            
            if not href or not title: continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–æ –Ω–∞—à–µ–º—É —Å–ø–∏—Å–∫—É
            is_target = any(re.search(pattern, title, re.IGNORECASE) for pattern in self.targets_cbr)
            
            if is_target:
                full_url = urljoin(base_url, href)
                
                # –ï—Å–ª–∏ —É–∂–µ –≤–∏–¥–µ–ª–∏ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                if full_url in self.processed_urls: continue
                
                print(f"üî• –ù–ê–ô–î–ï–ù –û–¢–ß–ï–¢ –¶–ë: {title}")
                
                # –ò—â–µ–º PDF –≤–Ω—É—Ç—Ä–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                pdf_url = None
                
                # –ò–Ω–æ–≥–¥–∞ —Å—Å—ã–ª–∫–∞ –≤–µ–¥–µ—Ç —Å—Ä–∞–∑—É –Ω–∞ —Ñ–∞–π–ª
                if href.lower().endswith('.pdf'):
                    pdf_url = full_url
                else:
                    # –ó–∞—Ö–æ–¥–∏–º –≤–Ω—É—Ç—Ä—å –Ω–æ–≤–æ—Å—Ç–∏
                    sub_soup = self.get_soup(full_url)
                    if sub_soup:
                        # –ò—â–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ PDF
                        pdf_link = sub_soup.find('a', href=re.compile(r'\.pdf$', re.IGNORECASE))
                        # –ò–Ω–æ–≥–¥–∞ –∫–Ω–æ–ø–∫–∞ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è "–°–∫–∞—á–∞—Ç—å"
                        if not pdf_link:
                            pdf_link = sub_soup.find('a', string=re.compile(r'–°–∫–∞—á–∞—Ç—å|–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç', re.IGNORECASE))
                            
                        if pdf_link:
                            pdf_url = urljoin(base_url, pdf_link['href'])

                if pdf_url:
                    text = self.extract_text_from_pdf(pdf_url)
                    if text:
                        analysis = self.analyze_with_gpt(text, title, "–ë–∞–Ω–∫–∞ –†–æ—Å—Å–∏–∏")
                        msg = f"üè¶ **–¶–ë –†–§: –ù–û–í–´–ô –û–¢–ß–ï–¢**\n\nüìÑ *{title}*\n\n{analysis}\n\nüîó [–ß–∏—Ç–∞—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª]({pdf_url})"
                        self.send_telegram(msg)
                        self.save_history(full_url)
                    else:
                        print("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ç–µ–∫—Å—Ç PDF")
                else:
                    print(f"PDF –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {full_url}")

    # --- –ü–†–û–í–ï–†–ö–ê –ú–ò–ù–≠–ö (–†–æ—Å—Å—Ç–∞—Ç –¥–∞–Ω–Ω—ã–µ) ---
    def check_minec(self):
        print("üîç [–ú–ò–ù–≠–ö] –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±–∑–æ—Ä–æ–≤...")
        base_url = "https://www.economy.gov.ru"
        # –†–∞–∑–¥–µ–ª –æ–±–∑–æ—Ä–æ–≤
        section_url = "https://www.economy.gov.ru/material/directions/makroec/ekonomicheskie_obzory/"
        
        soup = self.get_soup(section_url)
        if not soup: return

        links = soup.find_all('a')
        
        for link in links:
            title = link.get_text(strip=True)
            href = link.get('href')
            
            if not href or not title: continue
            
            is_target = any(re.search(pattern, title, re.IGNORECASE) for pattern in self.targets_minec)
            
            if is_target:
                full_url = urljoin(base_url, href)
                if full_url in self.processed_urls: continue
                
                print(f"üî• –ù–ê–ô–î–ï–ù –û–ë–ó–û–† –ú–ò–ù–≠–ö–ê: {title}")
                
                # –ó–∞—Ö–æ–¥–∏–º –≤–Ω—É—Ç—Ä—å
                sub_soup = self.get_soup(full_url)
                if sub_soup:
                    # –ò—â–µ–º PDF
                    pdf_link = sub_soup.find('a', href=re.compile(r'\.pdf$', re.IGNORECASE))
                    if pdf_link:
                        pdf_url = urljoin(base_url, pdf_link['href'])
                        text = self.extract_text_from_pdf(pdf_url)
                        
                        if text:
                            analysis = self.analyze_with_gpt(text, title, "–ú–∏–Ω—ç–∫–æ–Ω–æ–º—Ä–∞–∑–≤–∏—Ç–∏—è")
                            msg = f"üìâ **–ú–ò–ù–≠–ö (–î–ê–ù–ù–´–ï –†–û–°–°–¢–ê–¢–ê)**\n\nüìÑ *{title}*\n\n{analysis}\n\nüîó [–ß–∏—Ç–∞—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª]({pdf_url})"
                            self.send_telegram(msg)
                            self.save_history(full_url)

    def run(self):
        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ
        self.check_cbr()
        self.check_minec()
        print("‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –°–∫—Ä–∏–ø—Ç –∑–∞—Å—ã–ø–∞–µ—Ç.")

if __name__ == "__main__":
    agent = MacroAgent()
    agent.run()
