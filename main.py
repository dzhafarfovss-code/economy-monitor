import os
import requests
from bs4 import BeautifulSoup
import pdfplumber
import io
import re
import urllib3
from urllib.parse import urljoin
import json
import time

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
TG_BOT_TOKEN = os.environ.get("TG_BOT_TOKEN")
TG_CHAT_ID = os.environ.get("TG_CHAT_ID")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class MacroAgent:
    def __init__(self):
        self.history_file = "history.json"
        self.processed_urls = self.load_history()
        
        # –ú–∞—Å–∫–∏—Ä—É–µ–º—Å—è –ø–æ–¥ –æ–±—ã—á–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –ø—Ä–∏—à–µ–¥—à–µ–≥–æ —Å –Ø–Ω–¥–µ–∫—Å–∞
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
            "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
            "Referer": "https://yandex.ru/"
        }

        # –ß–¢–û –ò–©–ï–ú (–ò–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤/—Å—Ç–∞—Ç–µ–π)
        self.targets_cbr = [
            r"–û–±–∑–æ—Ä —Ä–∏—Å–∫–æ–≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ä—ã–Ω–∫–æ–≤",
            r"–†–µ–≥–∏–æ–Ω–∞–ª—å–Ω–∞—è —ç–∫–æ–Ω–æ–º–∏–∫–∞",
            r"–ú–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π –æ–ø—Ä–æ—Å",
            r"–î–µ–Ω–µ–∂–Ω–æ-–∫—Ä–µ–¥–∏—Ç–Ω—ã–µ —É—Å–ª–æ–≤–∏—è",
            r"–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ—Ç—Ä–∞—Å–ª–µ–≤—ã—Ö —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –ø–æ—Ç–æ–∫–æ–≤",
            r"–î–æ–∫–ª–∞–¥ –æ –¥–µ–Ω–µ–∂–Ω–æ-–∫—Ä–µ–¥–∏—Ç–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–µ"
        ]
        
        self.targets_minec = [
            r"–û —Ç–µ–∫—É—â–µ–π —Å–∏—Ç—É–∞—Ü–∏–∏",
            r"–ö–∞—Ä—Ç–∏–Ω–∞ –¥–µ–ª–æ–≤–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏",
            r"–≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π –æ–±–∑–æ—Ä"
        ]

    def load_history(self):
        if os.path.exists(self.history_file):
            try:
                with open(self.history_file, 'r') as f:
                    return set(json.load(f))
            except:
                return set()
        return set()

    def save_history(self, url):
        self.processed_urls.add(url)
        with open(self.history_file, 'w') as f:
            json.dump(list(self.processed_urls), f)

    def send_telegram(self, message):
        if not TG_BOT_TOKEN or not TG_CHAT_ID:
            print("!!! TG Keys missing")
            return

        for chunk in [message[i:i+4000] for i in range(0, len(message), 4000)]:
            url = f"https://api.telegram.org/bot{TG_BOT_TOKEN}/sendMessage"
            data = {"chat_id": TG_CHAT_ID, "text": chunk, "parse_mode": "Markdown"}
            try:
                requests.post(url, data=data, timeout=10)
                time.sleep(1)
            except Exception as e:
                print(f"TG Error: {e}")

    def get_soup(self, url, timeout=30):
        try:
            resp = requests.get(url, headers=self.headers, verify=False, timeout=timeout)
            resp.raise_for_status()
            resp.encoding = resp.apparent_encoding
            return BeautifulSoup(resp.text, 'html.parser')
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ ({url}): {e}")
            return None

    def extract_text_from_pdf(self, pdf_url):
        print(f"‚¨áÔ∏è PDF: {pdf_url}")
        try:
            resp = requests.get(pdf_url, headers=self.headers, verify=False, timeout=60)
            with pdfplumber.open(io.BytesIO(resp.content)) as pdf:
                text = ""
                # –ß–∏—Ç–∞–µ–º –ø–µ—Ä–≤—ã–µ 6 —Å—Ç—Ä–∞–Ω–∏—Ü
                for i in range(min(6, len(pdf.pages))):
                    t = pdf.pages[i].extract_text()
                    if t: text += t + "\n"
                return text
        except Exception as e:
            print(f"PDF Error: {e}")
            return None

    def analyze_with_gpt(self, text, title, source_name):
        if not OPENAI_API_KEY:
            return "‚ö†Ô∏è AI Key missing. Text start:\n" + text[:500]

        print("üß† GPT –¥—É–º–∞–µ—Ç...")
        try:
            from openai import OpenAI
            client = OpenAI(api_key=OPENAI_API_KEY)

            prompt = f"""
            –¢—ã ‚Äî –º–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –¥–ª—è —Ä—ã–Ω–∫–∞ –û–§–ó.
            –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–æ–∫—É–º–µ–Ω—Ç: "{title}" ({source_name}).
            
            –í—ã–¥–µ–ª–∏ –¢–û–õ–¨–ö–û —Å—É—Ç—å –¥–ª—è —Ç—Ä–µ–π–¥–µ—Ä–∞:
            1. ü¶Ö **–†–∏—Ç–æ—Ä–∏–∫–∞:** (–ñ–µ—Å—Ç–∫–∞—è/–ú—è–≥–∫–∞—è/–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è) + –ê—Ä–≥—É–º–µ–Ω—Ç.
            2. üìä **–§–∞–∫—Ç—ã:** (–ò–Ω—Ñ–ª—è—Ü–∏—è, –æ–∂–∏–¥–∞–Ω–∏—è, –¥–µ—Ñ–∏—Ü–∏—Ç –∫–∞–¥—Ä–æ–≤, –±—é–¥–∂–µ—Ç).
            3. üèõ **–û–§–ó:** (–ü–æ–∫—É–ø–∞—Ç—å/–ü—Ä–æ–¥–∞–≤–∞—Ç—å/–î–µ—Ä–∂–∞—Ç—å).
            
            –¢–µ–∫—Å—Ç (–Ω–∞—á–∞–ª–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞):
            {text[:11000]}
            """

            response = client.chat.completions.create(
                model="gpt-4o", 
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"AI Error: {e}"

    def check_cbr(self):
        print("üîç [–¶–ë] –ü—Ä–æ–≤–µ—Ä–∫–∞...")
        # –£–±—Ä–∞–ª–∏ –±–∏—Ç—ã–µ —Å—Å—ã–ª–∫–∏, –æ—Å—Ç–∞–≤–∏–ª–∏ –Ω–∞–¥–µ–∂–Ω—ã–µ
        urls = [
            "https://www.cbr.ru/calendar", 
            "https://www.cbr.ru/analytics/"
        ]
        
        for base_url in urls:
            soup = self.get_soup(base_url)
            if not soup: continue

            links = soup.find_all('a')
            for link in links:
                title = link.get_text(strip=True)
                href = link.get('href')
                
                if not href or not title: continue
                
                # –§–ò–õ–¨–¢–†: –¢–æ–ª—å–∫–æ 2025 –≥–æ–¥ (—á—Ç–æ–±—ã –Ω–µ –∫–∞—á–∞—Ç—å —Å—Ç–∞—Ä—å–µ)
                if "2025" not in title and "2025" not in href: continue

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏—è
                is_target = any(re.search(p, title, re.IGNORECASE) for p in self.targets_cbr)
                
                if is_target:
                    full_url = urljoin("https://www.cbr.ru", href)
                    
                    if full_url in self.processed_urls: continue
                    
                    print(f"üî• –ù–ê–ô–î–ï–ù: {title}")
                    
                    # –õ–æ–≥–∏–∫–∞ –ø–æ–∏—Å–∫–∞ PDF
                    pdf_url = None
                    if href.lower().endswith('.pdf'):
                        pdf_url = full_url
                    else:
                        # –ó–∞—Ö–æ–¥–∏–º –≤–Ω—É—Ç—Ä—å —Å—Ç–∞—Ç—å–∏
                        sub = self.get_soup(full_url)
                        if sub:
                            # –ò—â–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ PDF –≤–Ω—É—Ç—Ä–∏
                            pl = sub.find('a', href=re.compile(r'\.pdf$', re.IGNORECASE))
                            if pl: pdf_url = urljoin("https://www.cbr.ru", pl['href'])
                    
                    if pdf_url:
                        text = self.extract_text_from_pdf(pdf_url)
                        if text:
                            ans = self.analyze_with_gpt(text, title, "–¶–ë –†–§")
                            self.send_telegram(f"üè¶ **–¶–ë –†–§**\n\nüìÑ {title}\n\n{ans}\nüîó {pdf_url}")
                            self.save_history(full_url)
                            # –î–∞–µ–º –ø–∞—É–∑—É, —á—Ç–æ–±—ã –¶–ë –Ω–µ –∑–∞–±–∞–Ω–∏–ª
                            time.sleep(5)

    def check_minec(self):
        print("üîç [–ú–ò–ù–≠–ö] –ü—Ä–æ–≤–µ—Ä–∫–∞...")
        url = "https://www.economy.gov.ru/material/directions/makroec/ekonomicheskie_obzory/"
        # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π —Ç–∞–π–º–∞—É—Ç
        soup = self.get_soup(url, timeout=40) 
        if not soup: return

        links = soup.find_all('a')
        for link in links:
            title = link.get_text(strip=True)
            href = link.get('href')
            if not href or not title: continue
            
            if "2025" not in title and "2025" not in href: continue

            is_target = any(re.search(p, title, re.IGNORECASE) for p in self.targets_minec)
            if is_target:
                full_url = urljoin("https://www.economy.gov.ru", href)
                if full_url in self.processed_urls: continue
                
                print(f"üî• –ù–ê–ô–î–ï–ù –ú–ò–ù–≠–ö: {title}")
                sub = self.get_soup(full_url, timeout=40)
                if sub:
                    pl = sub.find('a', href=re.compile(r'\.pdf$', re.IGNORECASE))
                    if pl:
                        p_url = urljoin("https://www.economy.gov.ru", pl['href'])
                        text = self.extract_text_from_pdf(p_url)
                        if text:
                            ans = self.analyze_with_gpt(text, title, "–ú–∏–Ω–≠–∫")
                            self.send_telegram(f"üìâ **–ú–ò–ù–≠–ö**\n\nüìÑ {title}\n\n{ans}\nüîó {p_url}")
                            self.save_history(full_url)

    def run(self):
        self.check_cbr()
        self.check_minec()
        print("‚úÖ –ì–æ—Ç–æ–≤–æ")

if __name__ == "__main__":
    MacroAgent().run()
